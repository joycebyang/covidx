{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install efficientnet-pytorch sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import models\n",
    "from dataset_generator import DatasetGenerator\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "image_size = 224\n",
    "\n",
    "class_to_idx = {\n",
    "    'normal': 0,\n",
    "    'pneumonia': 1,\n",
    "    'COVID-19': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "0.5.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'data'\n",
    "train_csv_file = 'newtrain_split.txt'\n",
    "test_csv_file = 'test_split.txt'\n",
    "valid_csv_file = 'valid_split.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your transforms for the training, validation, and testing sets\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomOrder([\n",
    "        transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15, resample=Image.BILINEAR),\n",
    "        transforms.RandomResizedCrop(image_size, scale=(0.9, 1.0)),\n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "test_transforms = valid_transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Load the datasets with DatasetGenerator\n",
    "train_dir = os.path.join(image_dir, 'train')\n",
    "train_dataset = DatasetGenerator(train_csv_file, train_dir, transform=train_transforms)\n",
    "image, label = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the datasets with DatasetGenerator\n",
    "test_dir = os.path.join(image_dir, 'test')\n",
    "test_dataset = DatasetGenerator(test_csv_file, test_dir, transform=test_transforms)\n",
    "image, label = next(iter(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Load the datasets with DatasetGenerator\n",
    "valid_dir = os.path.join(image_dir,'train')\n",
    "valid_dataset = DatasetGenerator(valid_csv_file, valid_dir, transform=valid_transforms)\n",
    "image,label = next(iter(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedCovidSampler(Sampler):\n",
    "    def __init__(self, train_noncovid_list, train_covid_list, batch_size, covid_percent):\n",
    "        self.batch_size = batch_size\n",
    "        self.train_noncovid_list = np.array(train_noncovid_list)\n",
    "        self.train_covid_list = np.array(train_covid_list)\n",
    "        self.covid_percent = covid_percent\n",
    "        self.covid_size = max(int(batch_size*covid_percent),1) \n",
    "        self.size = len(self.train_noncovid_list) + len(self.train_covid_list)\n",
    "\n",
    "    def __do_batch__(self):\n",
    "        covid_inds = np.random.choice(self.train_covid_list, size=self.covid_size, replace=False)\n",
    "        noncovid_inds = np.random.choice(self.train_noncovid_list, size=(self.batch_size-self.covid_size), replace=False)\n",
    "        batch_inds = np.concatenate((covid_inds,noncovid_inds),axis=None)\n",
    "        np.random.shuffle(batch_inds)\n",
    "        \n",
    "        return batch_inds\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch_idx in range(0, self.size, self.batch_size):\n",
    "            yield self.__do_batch__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of indices for each class \n",
    "train_noncovid_list = []\n",
    "train_covid_list = []\n",
    "\n",
    "for index in range(len(train_dataset.csv_df)):\n",
    "    label = train_dataset.csv_df.iloc[index, 2]\n",
    "    if label == 'COVID-19':\n",
    "        train_covid_list.append(index)\n",
    "    else:\n",
    "        train_noncovid_list.append(index)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_percent = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "covid_size = max(int(batch_size*covid_percent),1)\n",
    "covid_inds = np.random.choice(np.array(train_covid_list), size=covid_size, replace=False)\n",
    "len(covid_inds)\n",
    "\n",
    "noncovid_size = batch_size - covid_size\n",
    "noncovid_inds = np.random.choice(np.array(train_noncovid_list), size=noncovid_size, replace=False)\n",
    "len(noncovid_inds)\n",
    "\n",
    "batch_inds = np.concatenate((covid_inds,noncovid_inds),axis=None)\n",
    "np.random.shuffle(batch_inds)\n",
    "print(batch_inds)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build and train your network\n",
    "def load_pretrained_model(arch):\n",
    "    model_func = getattr(models, arch)\n",
    "    model = model_func()\n",
    "    model.arch = arch\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert labels to tensor\n",
    "def labels_to_tensor(labels):\n",
    "    label_indices = [class_to_idx[label] for label in labels]\n",
    "    label_indices = np.array(label_indices, int)\n",
    "    label_indices = torch.LongTensor(label_indices)\n",
    "    return label_indices\n",
    "\n",
    "labels_to_tensor(['normal', 'pneumonia', 'COVID-19'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, valid_dataloader, loss_func, device):\n",
    "    #track accuracy and loss \n",
    "    accuracy = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad(): #deactivates requires_grad flag, disables tracking of gradients \n",
    "        for images, labels in valid_dataloader: #iterate over images and labels in valid dataset\n",
    "            labels = labels_to_tensor(labels)\n",
    "            images, labels = images.to(device), labels.to(device) #move a tensor to a device\n",
    "            log_ps = model.forward(images) #log form of probabilities for each label\n",
    "            test_loss += loss_func(log_ps, labels).item() #.item() returns loss value as float, compare prob to actual \n",
    "            \n",
    "            ps = torch.exp(log_ps) #gets rid of log \n",
    "            equality = (labels.data == ps.max(dim=1)[1]) #takes highest probability\n",
    "            accuracy += torch.mean(equality.type(torch.FloatTensor))\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do validation on the test set\n",
    "def test_model(model, test_loader, device):\n",
    "    #track accuracy, move to device, switch on eval mode\n",
    "    accuracy = 0\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in iter(test_loader):\n",
    "            labels = labels_to_tensor(labels)\n",
    "            images, labels = images.to(device), labels.to(device) #move a tensor to a device\n",
    "            log_ps = model.forward(images)\n",
    "            ps = torch.exp(log_ps)\n",
    "            \n",
    "            equality = (labels.data == ps.max(dim=1)[1])\n",
    "            accuracy += equality.type(torch.FloatTensor).mean()\n",
    "        model_accuracy = accuracy/len(test_loader)\n",
    "        model.accuracy = model_accuracy\n",
    "    \n",
    "    return model.accuracy           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save the checkpoint\n",
    "def save_checkpoint(checkpoint_path, model):\n",
    "    checkpoint = {\n",
    "        #TODO: Save the model arch, accuracy, classifier, class_to_idx\n",
    "        'arch':model.arch,\n",
    "        'accuracy':model.accuracy,\n",
    "        'class_to_idx':class_to_idx,\n",
    "        'state_dict':model.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print('Saved the trained model: %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "batch_size = 64\n",
    "weighted_covid_sampler = WeightedCovidSampler(train_noncovid_list,train_covid_list,batch_size,covid_percent)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=weighted_covid_sampler, num_workers=16)\n",
    "dataloaders = {\"train\": train_loader,\n",
    "               \"test\": DataLoader(test_dataset, batch_size=batch_size, num_workers=16, shuffle=True),\n",
    "              \"valid\":DataLoader(valid_dataset, batch_size=batch_size, num_workers=16, shuffle=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for _, labels in train_loader:\n",
    "    label_cnt_dict = {\n",
    "        'normal': 0,\n",
    "        'pneumonia': 0,\n",
    "        'COVID-19': 0\n",
    "    }\n",
    "\n",
    "    for label in labels:\n",
    "        label_cnt_dict[label] += 1\n",
    "    print(label_cnt_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DenseNet121',\n",
       " 'DenseNet169',\n",
       " 'DenseNet201',\n",
       " 'EfficientNet4',\n",
       " 'EfficientNet5',\n",
       " 'EfficientNet6',\n",
       " 'ResNet101',\n",
       " 'ResNet34',\n",
       " 'ResNet50']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models = [m for m in dir(models) if 'Net' in m and 'Model' not in m]\n",
    "available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b5-b6417697.pth\" to /home/ubuntu/.cache/torch/checkpoints/efficientnet-b5-b6417697.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca0a05c6e3b48869dd6a2e10915336f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=122410125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "arch = 'EfficientNet5'\n",
    "pretrained_model = load_pretrained_model(arch)\n",
    "optimizer = optim.Adam(pretrained_model.get_optimizer_parameters(),\n",
    "                       lr=0.00001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_func, optimizer, dataloaders, device, epochs, checkpoint_prefix, print_every=20):\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "\n",
    "    train_loader = dataloaders['train']\n",
    "    valid_loader = dataloaders['valid']\n",
    "\n",
    "    epoch_start = time.time()\n",
    "    max_acc = 0.0\n",
    "\n",
    "    # loop to train for number of epochs\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        batch_start = time.time()\n",
    "        steps = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            # within each loop, iterate train_loader, and print loss\n",
    "            label_indices = labels_to_tensor(labels)\n",
    "            images, labels = images.to(device), label_indices.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            log_ps = model.forward(images)\n",
    "            loss = loss_func(log_ps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                model.eval()\n",
    "                valid_loss, valid_accuracy = validate(model, valid_loader, loss_func, device)\n",
    "                model.train()\n",
    "                batch_time = time.time() - batch_start\n",
    "                print(\n",
    "                    \"Epoch: {}/{}..\".format(e + 1, epochs),\n",
    "                    \"Step: {}..\".format(steps),\n",
    "                    \"Training Loss: {:.3f}..\".format(running_loss / len(train_loader)),\n",
    "                    \"Test Loss: {:.3f}..\".format(valid_loss / len(valid_loader)),\n",
    "                    \"Test Accuracy: {:.3f}..\".format(valid_accuracy / len(valid_loader)),\n",
    "                    \"Batch Time: {:.3f}, avg: {:.3f}\".format(batch_time, batch_time / steps)\n",
    "                )\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss, valid_accuracy = validate(model, valid_loader, loss_func, device)\n",
    "        model.train()\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        if valid_accuracy > max_acc:\n",
    "            max_acc = valid_accuracy\n",
    "            model.accuracy = valid_accuracy\n",
    "            save_checkpoint('%s.pth.tar' % checkpoint_prefix, model)\n",
    "            print ('Epoch [{}] [save] Accuracy={:.3f} time: {:.3f}, avg: {:.3f}'\n",
    "                   .format(e + 1, valid_accuracy / len(valid_loader), epoch_time, epoch_time / (e + 1)))\n",
    "        else:\n",
    "            print ('Epoch [{}] [----] Accuracy={:.3f} time: {:.3f}, avg: {:.3f}'\n",
    "                   .format(e + 1, valid_accuracy / len(valid_loader), epoch_time, epoch_time / (e + 1)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#device = 'cpu'\n",
    "epochs = 2\n",
    "training_start = time.time()\n",
    "checkpoint_prefix = os.path.join('checkpoints','%s-%d'%(arch, training_start))\n",
    "    \n",
    "trained_model = train(pretrained_model, criterion, optimizer, dataloaders, device, epochs, checkpoint_prefix, print_every=20)\n",
    "print('%.2f seconds taken for model training' % (time.time() - training_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = dataloaders['test']\n",
    "test_accuracy = test_model(trained_model, test_loader, device)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
